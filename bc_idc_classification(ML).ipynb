{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.7.10",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "colab": {
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "EyDlJj2teZBX",
        "trusted": true
      },
      "source": [
        "#!/usr/bin/env python2\n",
        "# -*- coding: utf-8 -*-\n",
        "\n",
        "# =============================================================================\n",
        "# MODULES IMPORT\n",
        "# =============================================================================\n",
        "from __future__ import print_function\n",
        "import sys\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "import cv2\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import auc\n",
        "from sklearn.metrics import roc_curve\n",
        "from sklearn.metrics import f1_score\n",
        "import scipy as sc\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import StratifiedShuffleSplit\n",
        "from sklearn.mixture import GaussianMixture\n",
        "from sklearn.cluster import MiniBatchKMeans\n",
        "from sklearn.decomposition import PCA\n",
        "import pickle\n",
        "from sklearn import preprocessing\n",
        "from sklearn.feature_selection import chi2\n",
        "from sklearn.feature_selection import SelectKBest\n",
        "from sklearn.model_selection import RepeatedStratifiedKFold\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ikNOocDbX3Ri"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TbgIWi2vehMw",
        "trusted": true
      },
      "source": [
        "\n",
        "def getFileList(root):\n",
        "    import os\n",
        "    l =[]\n",
        "    for path, dirs, files in os.walk(root):\n",
        "         [l.append(os.path.join(path, f)) for f in files]\n",
        "    return l\n",
        "\n",
        "\n",
        "def sortTarget(im_folder, im_info):\n",
        "    import numpy as np\n",
        "\n",
        "    targets = np.empty(shape=im_folder.shape)\n",
        "    idx = 0\n",
        "    for filename in im_folder:\n",
        "        targets[idx] = np.where(im_info.filename == filename.split('/')[-1])[0].astype(int)\n",
        "        idx+=1\n",
        "\n",
        "    return im_info.loc[targets,:]\n",
        "\n",
        "\n",
        "def ReadImage(filename):\n",
        "    import cv2\n",
        "    return cv2.imread(filename)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "PSHH3xN_Xtnp"
      },
      "source": [
        "# source = https://youtu.be/yUrwEYgZUsA\n",
        "\n",
        "def normaalize(image):\n",
        "\n",
        "\n",
        "    img=cv2.imread(image, 1)\n",
        "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "    Io = 240 # Transmitted light intensity, Normalizing factor for image intensities\n",
        "    alpha = 1  #As recommend in the paper. tolerance for the pseudo-min and pseudo-max (default: 1)\n",
        "    beta = 0.15 #As recommended in the paper. OD threshold for transparent pixels (default: 0.15)\n",
        "\n",
        "\n",
        "    ######## Step 1: Convert RGB to OD ###################\n",
        "    ## reference H&E OD matrix.\n",
        "    #Can be updated if you know the best values for your image.\n",
        "    #Otherwise use the following default values.\n",
        "    #Read the above referenced papers on this topic.\n",
        "    HERef = np.array([[0.5626, 0.2159],\n",
        "                      [0.7201, 0.8012],\n",
        "                      [0.4062, 0.5581]])\n",
        "    ### reference maximum stain concentrations for H&E\n",
        "    maxCRef = np.array([1.9705, 1.0308])\n",
        "\n",
        "\n",
        "    # extract the height, width and num of channels of image\n",
        "    h, w, c = img.shape\n",
        "\n",
        "    # reshape image to multiple rows and 3 columns.\n",
        "    #Num of rows depends on the image size (wxh)\n",
        "    img = img.reshape((-1,3))\n",
        "\n",
        "    # calculate optical density\n",
        "    # OD = −log10(I)\n",
        "    #OD = -np.log10(img+0.004)  #Use this when reading images with skimage\n",
        "    #Adding 0.004 just to avoid log of zero.\n",
        "\n",
        "    OD = -np.log10((img.astype(np.float)+1)/Io) #Use this for opencv imread\n",
        "    #Add 1 in case any pixels in the image have a value of 0 (log 0 is indeterminate)\n",
        "\n",
        "    \"\"\"\n",
        "    from mpl_toolkits.mplot3d import Axes3D\n",
        "    fig = plt.figure(figsize=(16, 8))\n",
        "    ax1 = fig.add_subplot(121, projection='3d')\n",
        "    ax1.scatter(img[:,0],img[:,1],img[:,2])\n",
        "    ax2 = fig.add_subplot(122, projection='3d')\n",
        "    ax2.scatter(OD[:,0],OD[:,1],OD[:,2])\n",
        "    plt.show()\n",
        "    \"\"\"\n",
        "\n",
        "    ############ Step 2: Remove data with OD intensity less than β ############\n",
        "    # remove transparent pixels (clear region with no tissue)\n",
        "    ODhat = OD[~np.any(OD < beta, axis=1)] #Returns an array where OD values are above beta\n",
        "    #Check by printing ODhat.min()\n",
        "\n",
        "    ############# Step 3: Calculate SVD on the OD tuples ######################\n",
        "    #Estimate covariance matrix of ODhat (transposed)\n",
        "    # and then compute eigen values & eigenvectors.\n",
        "    eigvals, eigvecs = np.linalg.eigh(np.cov(ODhat.T))\n",
        "\n",
        "\n",
        "    ######## Step 4: Create plane from the SVD directions with two largest values ######\n",
        "    #project on the plane spanned by the eigenvectors corresponding to the two\n",
        "    # largest eigenvalues\n",
        "    That = ODhat.dot(eigvecs[:,1:3]) #Dot product\n",
        "\n",
        "    ############### Step 5: Project data onto the plane, and normalize to unit length ###########\n",
        "    ############## Step 6: Calculate angle of each point wrt the first SVD direction ########\n",
        "    #find the min and max vectors and project back to OD space\n",
        "    phi = np.arctan2(That[:,1],That[:,0])\n",
        "\n",
        "    minPhi = np.percentile(phi, alpha)\n",
        "    maxPhi = np.percentile(phi, 100-alpha)\n",
        "\n",
        "    vMin = eigvecs[:,1:3].dot(np.array([(np.cos(minPhi), np.sin(minPhi))]).T)\n",
        "    vMax = eigvecs[:,1:3].dot(np.array([(np.cos(maxPhi), np.sin(maxPhi))]).T)\n",
        "\n",
        "\n",
        "    # a heuristic to make the vector corresponding to hematoxylin first and the\n",
        "    # one corresponding to eosin second\n",
        "    if vMin[0] > vMax[0]:\n",
        "        HE = np.array((vMin[:,0], vMax[:,0])).T\n",
        "\n",
        "    else:\n",
        "        HE = np.array((vMax[:,0], vMin[:,0])).T\n",
        "\n",
        "\n",
        "    # rows correspond to channels (RGB), columns to OD values\n",
        "    Y = np.reshape(OD, (-1, 3)).T\n",
        "\n",
        "    # determine concentrations of the individual stains\n",
        "    C = np.linalg.lstsq(HE,Y, rcond=None)[0]\n",
        "\n",
        "    # normalize stain concentrations\n",
        "    maxC = np.array([np.percentile(C[0,:], 99), np.percentile(C[1,:],99)])\n",
        "    tmp = np.divide(maxC,maxCRef)\n",
        "    C2 = np.divide(C,tmp[:, np.newaxis])\n",
        "\n",
        "    ###### Step 8: Convert extreme values back to OD space\n",
        "    # recreate the normalized image using reference mixing matrix\n",
        "\n",
        "    Inorm = np.multiply(Io, np.exp(-HERef.dot(C2)))\n",
        "    Inorm[Inorm>255] = 254\n",
        "    Inorm = np.reshape(Inorm.T, (h, w, 3)).astype(np.uint8)\n",
        "\n",
        "    # Separating H and E components\n",
        "\n",
        "    H = np.multiply(Io, np.exp(np.expand_dims(-HERef[:,0], axis=1).dot(np.expand_dims(C2[0,:], axis=0))))\n",
        "    H[H>255] = 254\n",
        "    H = np.reshape(H.T, (h, w, 3)).astype(np.uint8)\n",
        "\n",
        "    E = np.multiply(Io, np.exp(np.expand_dims(-HERef[:,1], axis=1).dot(np.expand_dims(C2[1,:], axis=0))))\n",
        "    E[E>255] = 254\n",
        "    E = np.reshape(E.T, (h, w, 3)).astype(np.uint8)\n",
        "\n",
        "    return Inorm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QRzZgT12hCzB",
        "trusted": true
      },
      "source": [
        "def show_sift_features(gray_img, color_img, kp):\n",
        "    return plt.imshow(cv2.drawKeypoints(gray_img, kp, color_img.copy()))\n",
        "\n",
        "   # kpimg = cv2.drawKeypoints(im_gray, kp, None, flags=cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)\n",
        "   # plt.imshow(kpimg)\n",
        "\n",
        "def computeFV(xx, gmm):\n",
        "    \"\"\"Computes the Fisher vector on a set of descriptors.\n",
        "    Parameters\n",
        "    ----------\n",
        "    xx: array_like, shape (N, D) or (D, )\n",
        "        The set of descriptors\n",
        "    gmm: instance of sklearn mixture.GMM object\n",
        "        Gauassian mixture model of the descriptors.\n",
        "    Returns\n",
        "    -------\n",
        "    fv: array_like, shape (K + 2 * D * K, )\n",
        "        Fisher vector (derivatives with respect to the mixing weights, means\n",
        "        and variances) of the given descriptors.\n",
        "    Reference\n",
        "    ---------\n",
        "    J. Krapac, J. Verbeek, F. Jurie.  Modeling Spatial Layout with Fisher\n",
        "    Vectors for Image Categorization.  In ICCV, 2011.\n",
        "    http://hal.inria.fr/docs/00/61/94/03/PDF/final.r1.pdf\n",
        "    \"\"\"\n",
        "    xx = np.atleast_2d(xx)\n",
        "    N = xx.shape[0]\n",
        "\n",
        "    # Compute posterior probabilities.\n",
        "    Q = gmm.predict_proba(xx)  # NxK\n",
        "\n",
        "    # Compute the sufficient statistics of descriptors.\n",
        "    Q_sum = np.sum(Q, 0)[:, np.newaxis] / N\n",
        "    Q_xx = np.dot(Q.T, xx) / N\n",
        "    Q_xx_2 = np.dot(Q.T, xx ** 2) / N\n",
        "\n",
        "    # Compute derivatives with respect to mixing weights, means and variances.\n",
        "    d_pi = Q_sum.squeeze() - gmm.weights_\n",
        "    d_mu = Q_xx - Q_sum * gmm.means_\n",
        "    d_sigma = (\n",
        "            - Q_xx_2\n",
        "            - Q_sum * gmm.means_ ** 2\n",
        "            + Q_sum * gmm.covariances_\n",
        "            + 2 * Q_xx * gmm.means_)\n",
        "\n",
        "    # Merge derivatives into a vector.\n",
        "    return np.hstack((d_pi, d_mu.flatten(), d_sigma.flatten()))\n",
        "\n",
        "\n",
        "\n",
        "def FeatureExtract(im_file, nkeys, pca, gmm, scaler):\n",
        "    # read image\n",
        "    im = normaalize(im_file)\n",
        "\n",
        "    # rgb normalization\n",
        "   # im = rgb_normalized(im)\n",
        "\n",
        "    # to gray\n",
        "    im_gray = cv2.cvtColor(im, cv2.COLOR_BGR2GRAY)\n",
        "    #im_gray = cv2.normalize(im_gray,im_gray, 0, 255, cv2.NORM_MINMAX)\n",
        "\n",
        "    # apply gaussian filter\n",
        "    #im_gray = cv2.GaussianBlur(im_gray,(15,15),0)\n",
        "\n",
        "    # extract SIFT descriptors\n",
        "    orb = cv2.ORB_create(nfeatures = nkeys)\n",
        "    kp, descriptors = orb.detectAndCompute(im_gray, None)\n",
        "    descriptors = np.array(descriptors, dtype=float)\n",
        "\n",
        "    descriptors /= (descriptors.sum(axis=1, keepdims=True) + 1e-7)\n",
        "    descriptors = np.sqrt(descriptors)\n",
        "\n",
        "    # apply scaler and pca transform\n",
        "    descriptors = scaler.transform(descriptors)\n",
        "    descriptors = pca.transform(descriptors)\n",
        "\n",
        "    # compute Fisher Vector\n",
        "    fv = computeFV(descriptors, gmm)\n",
        "\n",
        "    # power-normalization\n",
        "   # fv = np.sign(fv) * np.abs(fv) ** 0.5\n",
        "    # L2 normalize\n",
        "    #fv /= np.sqrt(np.sum(fv ** 2))\n",
        "\n",
        "    return fv"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iXlylptAeZBq",
        "trusted": true
      },
      "source": [
        "#==============================================================================\n",
        "# IMAGE PARAMETERS\n",
        "#==============================================================================\n",
        "path_train = '/content/drive/MyDrive/BC_IDC/4x/train_uncat_4x'\n",
        "path_test = '/content/drive/MyDrive/BC_IDC/4x/test_uncat_4x'\n",
        "\n",
        "test_perc = 0.2 # test set percentage\n",
        "\n",
        "#==============================================================================\n",
        "# GET IMAGE LIST AND INFO\n",
        "#==============================================================================\n",
        "im_folder_train = np.array(getFileList(path_train)) # image list\n",
        "im_folder_test = np.array(getFileList(path_test))\n",
        "\n",
        "# Load csv with image information\n",
        "im_info_train = pd.read_csv('/content/drive/MyDrive/BC_IDC/4x/train_labels_4x.csv')\n",
        "im_info_test = pd.read_csv('/content/drive/MyDrive/BC_IDC/4x/test_labels_4x.csv')\n",
        "\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# MATCH IMAGE LIST AND LABELS\n",
        "# =============================================================================\n",
        "\n",
        "im_info_train = sortTarget(im_folder_train,im_info_train)\n",
        "im_info_test = sortTarget(im_folder_test,im_info_test)\n",
        "\n",
        "le = preprocessing.LabelEncoder()\n",
        "T_train = im_info_train.target\n",
        "T_train = np.array(le.fit_transform(T_train))\n",
        "\n",
        "T_test = im_info_test.target\n",
        "T_test = np.array(le.fit_transform(T_test))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LsWn5n7deZBs",
        "trusted": true
      },
      "source": [
        "# =============================================================================\n",
        "# TRAIN/TEST SPLIT\n",
        "# =============================================================================\n",
        "\n",
        "y_train = T_train\n",
        "y_test = T_test\n",
        "\n",
        "\n",
        "print('Train set size', y_train.shape[0], 'images')\n",
        "print('Test set size', y_test.shape[0], 'images')\n",
        "print(y_test)\n",
        "print(y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FN6ea9pTeZBt",
        "trusted": true
      },
      "source": [
        "# =============================================================================\n",
        "# FISHER VECTOR PARAMETERS\n",
        "# =============================================================================\n",
        "n_cmp = 10 # pca components\n",
        "k = 256 # gmm n centroids\n",
        "fnum = 8192 # n sift descriptors"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XW3bS86h1kHc",
        "trusted": true
      },
      "source": [
        "!pip install opencv-python==3.4.2.17\n",
        "!pip install opencv-contrib-python==3.4.2.17"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HwVu7ifeeZBu",
        "trusted": true
      },
      "source": [
        "# ============================================================================\n",
        "# EXTRACT SIFT DESCRIPTORS FROM TRAIN SET\n",
        "# =============================================================================\n",
        "dictionary = []\n",
        "for file  in tqdm(im_folder_train):\n",
        "    im = normaalize(file)\n",
        "    im_gray = cv2.cvtColor(im, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "    # Extract sift descriptors\n",
        "    orb = cv2.ORB_create(nfeatures = fnum)\n",
        "    kp, descriptors = orb.detectAndCompute(im_gray, None)\n",
        "    descriptors = np.array(descriptors, dtype=float)\n",
        "    descriptors /= (descriptors.sum(axis=1, keepdims=True) + 1e-7)\n",
        "    descriptors = np.sqrt(descriptors)\n",
        "\n",
        "    dictionary.append(descriptors)\n",
        "\n",
        "\n",
        "dictionary = np.asarray(dictionary)\n",
        "dictionary = np.concatenate(dictionary).astype(None)\n",
        "print(descriptors)\n",
        "print(dictionary)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wDwwjf_SKlAv"
      },
      "source": [
        "\n",
        "#with open(\"/content/drive/MyDrive/BreastCancer-Classifier/descrip_norm.pkl\", 'wb') as d:\n",
        "\n",
        "    #pickle.dump(descriptors, d, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "#with open(\"/content/drive/MyDrive/BreastCancer-Classifier/descrip.pkl\", 'rb') as d1:\n",
        "    #descriptors = pickle.load(d1)\n",
        "\n",
        "#print(descriptors)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EwXNxcymDGgr",
        "trusted": true
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "#with open(\"/content/drive/MyDrive/BreastCancer-Classifier/dict_norm_E_H.pkl\", 'wb') as f:\n",
        "    # indent=2 is not needed but makes the file human-readable\n",
        "    #pickle.dump(dictionary, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "#with open(\"/content/drive/MyDrive/BreastCancer-Classifier/dict.pkl\", 'rb') as f1:\n",
        "    #dictionary = pickle.load(f1)\n",
        "\n",
        "print(dictionary.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "feXlh8eIeZBv",
        "trusted": true
      },
      "source": [
        "# =============================================================================\n",
        "# APPLY PCA TO DESCRIPTORS LIBRARY\n",
        "# =============================================================================\n",
        "sift_scaler = preprocessing.StandardScaler()\n",
        "descriptors = sift_scaler.fit_transform(descriptors)\n",
        "\n",
        "sift_pca = PCA(n_components=n_cmp,whiten=True)\n",
        "dictionary = sift_pca.fit_transform(dictionary)\n",
        "dictionary = np.float32(dictionary)\n",
        "\n",
        "with open('/content/drive/MyDrive/BC_IDC/4x/pca_transform_norm.pickle', 'wb') as handle:\n",
        "    pickle.dump(sift_pca, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "with open('/content/drive/MyDrive/BC_IDC/4x/scaler_norm.pickle', 'wb') as handle:\n",
        "    pickle.dump(sift_scaler, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "with open(\"/content/drive/MyDrive/BC_IDC/4x/my_dict_norm_H.pkl\", 'wb') as f:\n",
        "\n",
        "    pickle.dump(dictionary, f, protocol=pickle.HIGHEST_PROTOCOL)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rddx5l_qQCC2",
        "trusted": true
      },
      "source": [
        "with open('/content/drive/MyDrive/40x/pca_transform_norm.pickle', 'rb') as handle:\n",
        "    sift_pca = pickle.load(handle)\n",
        "with open('/content/drive/MyDrive/40x/scaler_norm.pickle', 'rb') as handle:\n",
        "    sift_scaler = pickle.load(handle)\n",
        "\n",
        "with open(\"/content/drive/MyDrive/40x/my_dict_norm_H.pkl\", 'rb') as f:\n",
        "    # indent=2 is not needed but makes the file human-readable\n",
        "    dictionary = pickle.load(f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jiHDA-g_eZBw",
        "trusted": true
      },
      "source": [
        "## =============================================================================\n",
        "## BUILD DICTIONARY MODEL\n",
        "## =============================================================================\n",
        "gmm_pca = GaussianMixture(n_components = k, covariance_type = \"diag\").fit(dictionary)\n",
        "with open(\"/content/drive/MyDrive/BC_IDC/4x/gmm_norm256_H.pkl\", 'wb') as f:\n",
        "\n",
        "    pickle.dump(gmm_pca, f, protocol=pickle.HIGHEST_PROTOCOL)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ac1Wh3v3eZBx"
      },
      "source": [
        "\n",
        "#with open('pca4.pickle', 'rb') as handle:\n",
        "#    sift_pca = pickle.load(handle)\n",
        "#with open('scaler4.pickle', 'rb') as handle:\n",
        "#    sift_scaler = pickle.load(handle)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b6D6GZCVO2Ft"
      },
      "source": [
        "#with open(\"/content/drive/MyDrive/BreastCancer-Classifier/gmm_norm256.pkl\", 'rb') as f:\n",
        "\n",
        "    #gmm_pca = pickle.load(f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wkTJ0qRYeZBy",
        "trusted": true
      },
      "source": [
        "# =============================================================================\n",
        " # COMPUTE FISHER VECTORS FOR TRAIN SET\n",
        "# =============================================================================\n",
        "X_train = np.empty((y_train.shape[0],k+2*dictionary.shape[1]*k))\n",
        "idx = 0\n",
        "for file in tqdm(im_folder_train):\n",
        "    X_train[idx,:] = FeatureExtract(file, nkeys = fnum, pca = sift_pca, gmm = gmm_pca, scaler = sift_scaler)\n",
        "    idx += 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "syre8MBEeZBz",
        "trusted": true
      },
      "source": [
        "# =============================================================================\n",
        " # COMPUTE FISHER VECTORS FOR TEST SET\n",
        "# =============================================================================\n",
        "X_test = np.empty((y_test.shape[0],k+2*dictionary.shape[1]*k))\n",
        "\n",
        "idx = 0\n",
        "for file in tqdm(im_folder_test):\n",
        "    X_test[idx,:] = FeatureExtract(file, nkeys = fnum, pca = sift_pca, gmm = gmm_pca, scaler = sift_scaler)\n",
        "    idx += 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S_ocQGOSVJHP",
        "trusted": true
      },
      "source": [
        "with open('./X_train_norm320H.pickle', 'wb') as handle:\n",
        "    pickle.dump(X_train, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "with open('./X_test_norm320H.pickle', 'wb') as handle:\n",
        "    pickle.dump(X_test, handle, protocol=pickle.HIGHEST_PROTOCOL)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K-DKNoKbeZB0",
        "trusted": true
      },
      "source": [
        "#with open('/content/drive/MyDrive/BreastCancer-Classifier/X_train.pickle', 'rb') as handle:\n",
        "    #X_train = pickle.load(handle)\n",
        "#with open('/content/drive/MyDrive/BreastCancer-Classifier/X_test.pickle', 'rb') as handle:\n",
        "    #X_test = pickle.load(handle)\n",
        "print(X_test.shape)\n",
        "print(X_train.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H1TVn3sYeZB0",
        "trusted": true
      },
      "source": [
        "# =============================================================================\n",
        "# PRE PROCESSING\n",
        "# =============================================================================\n",
        "#ch2 = SelectKBest(chi2, k=100)\n",
        "#X_train = ch2.fit_transform(X_train, y_train)\n",
        "#X_test = ch2.transform(X_test)\n",
        "#\n",
        "\n",
        "##PCA\n",
        "pca = PCA(n_components = 20, whiten=True,random_state=42)\n",
        "X_train = pca.fit_transform(X_train)\n",
        "X_test = pca.transform(X_test)\n",
        "#\n",
        "## SCALING data\n",
        "#scaler = preprocessing.MinMaxScaler(feature_range=(-1, 1), copy=True)\n",
        "scaler = preprocessing.StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4uUXYCwneZB1",
        "trusted": true
      },
      "source": [
        "# =============================================================================\n",
        "# SVM MODEL parameters\n",
        "# =============================================================================\n",
        "sss = RepeatedStratifiedKFold(n_splits=5, n_repeats=200, random_state=42)\n",
        "\n",
        "C_range = 2. ** np.arange(0, 1, step=0.05) # finer search\n",
        "g_range = np.logspace(-2, -1, 20)\n",
        "\n",
        "tuned_parameters = [{'kernel': ['rbf'], 'gamma': g_range, 'C': C_range}]\n",
        "#tuned_parameters = [{'kernel': ['linear'],  'C': C_range}]\n",
        "#tuned_parameters = [{'kernel': ['poly'],  'C': C_range,'degree': [2,3,4,5,6,7,8]}]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BSIaJJufeZB1",
        "trusted": true
      },
      "source": [
        "# ==============================================================================\n",
        " # GRID SEARCH\n",
        "# ==============================================================================\n",
        "scores = ['f1']\n",
        "\n",
        "for score in scores:\n",
        "    print(\"# Tuning hyper-parameters for %s\" % score)\n",
        "    print()\n",
        "\n",
        "    clf = GridSearchCV(SVC(cache_size=2000, random_state = 42, decision_function_shape='ovr'), tuned_parameters, cv=sss,\n",
        "                       scoring='%s_macro' % score, n_jobs=-1)\n",
        "\n",
        "# =============================================================================\n",
        "#     COMPUTE PARAMETERS\n",
        "# =============================================================================\n",
        "    t2 = time.time()\n",
        "    clf.fit(X_train, y_train)\n",
        "    elapsed2 = time.time() - t2\n",
        "    print()\n",
        "    print(\"Best parameters set found on development set:\")\n",
        "    print(clf.best_params_)\n",
        "    print()\n",
        "\n",
        "    print('Training time: ', elapsed2)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J7imBVk9eZB2",
        "trusted": true
      },
      "source": [
        "# EVALUATION\n",
        "# ==============================================================================\n",
        "# TRAIN SET\n",
        "# ===========================================================================\n",
        "clf2 = clf.best_estimator_\n",
        "#s print(clf2)\n",
        "print(\"Classification on training set:\")\n",
        "y_true, y_pred = y_train, clf2.predict(X_train)\n",
        "print('Confusion matrix:')\n",
        "print(confusion_matrix(y_true, y_pred))\n",
        "print(\" Train set f1 score: \" + str(f1_score(y_true, y_pred, average='macro')))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FWcWHr7yeZB3",
        "trusted": true
      },
      "source": [
        "# ==============================================================================\n",
        "# TESTING\n",
        "# ==============================================================================\n",
        "y_true, y_pred = y_test, clf2.predict(X_test)\n",
        "y_pred_ci = clf.decision_function(X_test)\n",
        "print(\"Classification on test set:\")\n",
        "print(classification_report(y_true, y_pred))\n",
        "print('Confusion matrix:')\n",
        "print(confusion_matrix(y_true, y_pred))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PPTiR9zdeZB4"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    }
  ]
}